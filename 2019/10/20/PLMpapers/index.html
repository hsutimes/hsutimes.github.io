<!DOCTYPE html>
<html lang="en">




<head><meta name="generator" content="Hexo 3.9.0">

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  
      <title>PLMpapers - Time</title>
  

  
  
  <meta name="description" content>
  <meta name="author" content="times">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- load loadjs.js -->
  <script src="/libs/loadjs/dist/loadjs.min.js"></script>

<link rel="stylesheet" href="/libs/animate.css/animate.min.css">
  <!-- load lightgallery -->
<link rel="stylesheet" href="/css/lightgallery.css">
<link rel="stylesheet" href="/libs/noty/lib/noty.css">
<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  






    <link rel="stylesheet" href="/css/taurus.css">
    
        <link rel="stylesheet" href="/css/scheme-taurus/animations.css">
    


<link rel="stylesheet" href="/.css">

  <!-- load font awesome 5 -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <!-- load mathjax -->
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax//libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <!-- load js-cookie -->
  <script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
    <script src="/js/social-share.min.js"></script>
    <script src="/js/theme.js"></script>

  <!-- include cookie.js -->
  
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-134224598-1', 'auto');
	ga('send', 'pageview');
</script>


  <!-- include comment system code -->
  
    <link rel="stylesheet" href="/css/gitment/default.css">
<script src="/js/gitment.browser.js"></script>
  
  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">
</head>
<body style="display: flex; flex-direction: column; min-height: 100vh;">

 

<header id="header" class='header'>
	<div class="header-title">
		
		<div class='header-logo'>
			<a href='/'>
				<img src='/images/theme-icon.svg' />
			</a>
		</div>
		<div class='header-text'>
			<h1>
				<a href="/">Time</a>
			</h1>
			<subtitle>
				time
			</subtitle>
		</div>
		
	</div>
	<div id='header-nav'>
		



<nav id="nav">
	
	
	
	<div class='nav-item' id='nav-item-toc'>
		


<div class='toc-container'>
<i class="far fa-times-circle" id='toc-close' onclick='closeTOC(event);' ontouchstart='closeTOC(event);'></i>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PLMpapers"><span class="toc-number">1.</span> <span class="toc-text">PLMpapers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Papers"><span class="toc-number">1.2.</span> <span class="toc-text">Papers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tutorial-amp-Resource"><span class="toc-number">1.3.</span> <span class="toc-text">Tutorial &amp; Resource</span></a></li></ol></li></ol>
</div>
<div class="toc-button" onclick='toggleTOC(event);' ontouchstart='toggleTOC(event);'>
    <img src="/images/icons/blue-shadow/toc.svg" alt="">
</div>

	</div>
	
	<div class='nav-item' id='nav-item-archive'>
		
				<div class='nav-icon'>
				
			<a href="/archives/" title='归档'>
			<img src="/images/icons/blue-shadow/archive.svg" alt="">
			</a>
		</div>
	</div>
	<div class='nav-item' id='nav-item-search'>
		
		<div class='nav-icon'>
		
			<a href="/search/" title='搜索'>
			<img src="/images/icons/blue-shadow/search.svg" alt="">
			</a>
		</div>
	</div>
	<div class="nav-item" id='nav-item-more'>
		<div class="nav-icon">
				<a href='#' onclick='onClickMenuIcon(event);' ontouchstart='onClickMenuIcon(event);'>
				<img src="/images/icons/blue-shadow/menu.svg" alt="">
				</a>
		</div>
		<div class="nav-more-menu">
				<i class="far fa-times-circle" id='nav-more-menu-close' onclick='onClickNavMenuClose(event);' ontouchstart='onClickNavMenuClose(event);'></i>
		
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/文章/">
						<span>文章</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/算法/">
						<span>算法</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/人工智能/">
						<span>人工智能</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/生活/">
						<span>生活</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/笔试/">
						<span>笔试</span>
					</a>
				</div>
		</div>
		
	</div>
	</div>
</nav>

	</div>
</header>

 

  




  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div style="flex: 1;">
      <style>
    body {
        background-color: white;
    }
</style>








    
        
            
            
        
    






    
    
        
    

    
        
    









<article class="article" id="/2019/10/20/PLMpapers/" data-name='PLMpapers' data-version="">

    <!-- Title -->
    <div class='article-header'>
         
         <h1 class='article-title'>
            <a href="/2019/10/20/PLMpapers/">
                PLMpapers
            </a>
        </h1>
        <!-- TODO: support nested categories,display them nicely -->
        
        <ul class='article-categories'>
            
            
                <li><a href="/categories/文章/" data-no-instant>
                    <img src="/images/文章.svg" alt="文章" onerror='if(this.src != "/images/uncategorized.svg") this.src="/images/uncategorized.svg"' title='文章'/>
                </a></li>
            
        </ul>
        
    </div>
    
    <!-- Date and Author -->
    <div class='article-meta'>
    <ul>
            <li><i class='fa fa-calendar'></i> 2019-10-20</li>
            
            <li class='comment-button'><a href='#article-comment'><i class='fa fa-comments'></i> <span id='article-comment-count'>0</span></a></li>
            <li><i class="fa fa-eye"></i> <span id='article-visit-count'>0</span></li>
            <li class='thumb-up-button' id='thumb-up-button'><i class="far fa-thumbs-up fa-lg" id='thumb-up-icon'></i> <span id='article-thumbup-count'>0</span></li>
            
            <li><i class="fa fa-user"></i> times</li>
            <li><i class="fas fa-copyright"></i>
            
                
                
            
            
                <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a>
            
            </li>
    </ul>
    
<div class="tags">
	
		
			<label class='tag-1'><a href="/tags/NLP/">Nlp</a></label>
		
	
	</div>

    </div>
    <div class='article-cards'>
        <!-- Author Card -->
        <!---
        <div class='Card-article Card-author'>
            <div class='card-title'>
                <h3></h3>
            </div>
            <div class='card-content'>
                    <div class="author-meta">
                            <div class='author-figure'>
                                <img src="" alt="">
                            </div>
                            <div class='author-name'>
                                times
                            </div>
                        </div>
                        <div class="author-ai">
                            <div class='author-intro'>
                                <!-- TODO: auto generating author description -->
                                <!-- 
                            </div>
                            <div class="author-articles">
                                <!-- TODO: auto generating author articles -->
                                <!-- <ul>
                                    <li>Article 1</li>
                                    <li>Article 2</li>
                                    <li>Article 3</li>
                                    <li>Article 4</li>
                                    <li>Article 5</li>
                                    <li>Article 6</li>
                                </ul>
                            </div>
                        </div>
            </div>
            
        </div> -->

        <!-- Visit Card -->
        <!-- <div class="Card-article Card-visit"> -->
            <!-- <div class="card-title">
  <h3>Post Visit</h3>
</div>
<div class="card-chart">
  <div id='chart-post-visit'></div>
</div> -->
        <!-- </div> -->
        
        <!-- Auto Excerpt Card -->
        <!-- <div class="Card-article Card-excerpt">
            <div class="card-title">
  <h3>Quick Read</h3>
</div>
<div class="card-text">
  <p id='text-post-summary'>PLMpapersContributed by Xiaozhi Wang and Zhengyan Zhang.
IntroductionPre-trained Languge Model (PLM) is a very popular topic in NLP. In this repo, we list some representative work on PLM and show their relationship with a diagram. Feel free to distribute or use it! Here you can get the source...</p>
</div>
        </div> -->
    </div>
    
    <!-- Gallery -->
    <!-- TODO: add a slider to gallery -->
    

    <!-- Content -->
    <!-- TODO: support table of content -->
    <div class="article-toc" id='article-toc'>
    
        


<div class='toc-container'>
<i class="far fa-times-circle" id='toc-close' onclick='closeTOC(event);' ontouchstart='closeTOC(event);'></i>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PLMpapers"><span class="toc-number">1.</span> <span class="toc-text">PLMpapers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Papers"><span class="toc-number">1.2.</span> <span class="toc-text">Papers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tutorial-amp-Resource"><span class="toc-number">1.3.</span> <span class="toc-text">Tutorial &amp; Resource</span></a></li></ol></li></ol>
</div>
<div class="toc-button" onclick='toggleTOC(event);' ontouchstart='toggleTOC(event);'>
    <img src="/images/icons/blue-shadow/toc.svg" alt="">
</div>

    </div>
    <div class='article-content'>
    <h1 id="PLMpapers"><a href="#PLMpapers" class="headerlink" title="PLMpapers"></a>PLMpapers</h1><p>Contributed by <a href="https://bakser.github.io/" target="_blank" rel="noopener">Xiaozhi Wang</a> and <a href="https://github.com/zzy14" target="_blank" rel="noopener">Zhengyan Zhang</a>.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Pre-trained Languge Model (PLM) is a very popular topic in NLP. In this repo, we list some representative work on PLM and show their relationship with a diagram. Feel free to distribute or use it! <a href="https://github.com/thunlp/PLMpapers/blob/master/PLMfamily.pptx" target="_blank" rel="noopener">Here</a> you can get the source PPT file of the diagram if you want to use it in your presentation.</p>
<p><img src="/images/2019/10/20/3f634050-f316-11e9-b017-0b2638cf8a9a.png" alt="image.png"></p>
<p>Corrections and suggestions are welcomed. </p>
<p>We also released <a href="https://github.com/thunlp/OpenCLaP" target="_blank" rel="noopener">OpenCLap</a>, an open-source Chinese language pre-trained model zoo. Welcome to try it.</p>
<h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><ol>
<li><strong>Semi-supervised Sequence Learning</strong>. <em>Andrew M. Dai, Quoc V. Le</em>. NIPS 2015. [<a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>context2vec: Learning Generic Context Embedding with Bidirectional LSTM</strong>. <em>Oren Melamud, Jacob Goldberger, Ido Dagan</em>. CoNLL 2016. [<a href="https://www.aclweb.org/anthology/K16-1006.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="http://u.cs.biu.ac.il/~nlp/resources/downloads/context2vec/" target="_blank" rel="noopener">project</a>] (<strong>context2vec</strong>)</li>
<li><strong>Unsupervised Pretraining for Sequence to Sequence Learning</strong>. <em>Prajit Ramachandran, Peter J. Liu, Quoc V. Le</em>. EMNLP 2017. [<a href="https://arxiv.org/pdf/1611.02683.pdf" target="_blank" rel="noopener">pdf</a>] (<strong>Pre-trained seq2seq</strong>)`</li>
<li><strong>Deep contextualized word representations</strong>. <em>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer</em>. NAACL 2018. [<a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://allennlp.org/elmo" target="_blank" rel="noopener">project</a>] (<strong>ELMo</strong>)</li>
<li><strong>Universal Language Model Fine-tuning for Text Classification</strong>. <em>Jeremy Howard and Sebastian Ruder</em>. ACL 2018. [<a href="https://www.aclweb.org/anthology/P18-1031" target="_blank" rel="noopener">pdf</a>] [<a href="http://nlp.fast.ai/category/classification.html" target="_blank" rel="noopener">project</a>] (<strong>ULMFiT</strong>)</li>
<li><strong>Improving Language Understanding by Generative Pre-Training</strong>. <em>Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever</em>. Preprint. [<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://openai.com/blog/language-unsupervised/" target="_blank" rel="noopener">project</a>] (<strong>GPT</strong>)</li>
<li><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>. <em>Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova</em>. NAACL 2019. [<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>Language Models are Unsupervised Multitask Learners</strong>. <em>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever</em>. Preprint. [<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/openai/gpt-2" target="_blank" rel="noopener">code</a>] (<strong>GPT-2</strong>)</li>
<li><strong>ERNIE: Enhanced Language Representation with Informative Entities</strong>. <em>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun and Qun Liu</em>. ACL 2019. [<a href="https://www.aclweb.org/anthology/P19-1139" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/thunlp/ERNIE" target="_blank" rel="noopener">code &amp; model</a>] (<strong>ERNIE (Tsinghua)</strong> )</li>
<li><strong>ERNIE: Enhanced Representation through Knowledge Integration</strong>. <em>Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian and Hua Wu</em>. Preprint. [<a href="https://arxiv.org/pdf/1904.09223.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/PaddlePaddle/ERNIE/tree/develop/ERNIE" target="_blank" rel="noopener">code</a>] (<strong>ERNIE (Baidu)</strong> )</li>
<li><strong>Defending Against Neural Fake News</strong>. <em>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi</em>. NeurIPS 2019. [<a href="https://arxiv.org/pdf/1905.12616.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://rowanzellers.com/grover/" target="_blank" rel="noopener">project</a>] (<strong>Grover</strong>)</li>
<li><strong>Cross-lingual Language Model Pretraining</strong>. <em>Guillaume Lample, Alexis Conneau</em>. NeurIPS 2019. [<a href="https://arxiv.org/pdf/1901.07291.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/facebookresearch/XLM" target="_blank" rel="noopener">code &amp; model</a>] (<strong>XLM</strong>)</li>
<li><strong>Multi-Task Deep Neural Networks for Natural Language Understanding</strong>. <em>Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao</em>. ACL 2019. [<a href="https://www.aclweb.org/anthology/P19-1441" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener">code &amp; model</a>] (<strong>MT-DNN</strong>)</li>
<li><strong>MASS: Masked Sequence to Sequence Pre-training for Language Generation</strong>. <em>Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu</em>. ICML 2019. [<a href="https://arxiv.org/pdf/1905.02450.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/microsoft/MASS" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>Unified Language Model Pre-training for Natural Language Understanding and Generation</strong>. <em>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon</em>. Preprint. [<a href="https://arxiv.org/pdf/1905.03197.pdf" target="_blank" rel="noopener">pdf</a>] (<strong>UniLM</strong>)</li>
<li><strong>XLNet: Generalized Autoregressive Pretraining for Language Understanding</strong>. <em>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le</em>. NeurIPS 2019. [<a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>RoBERTa: A Robustly Optimized BERT Pretraining Approach</strong>. <em>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov</em>. Preprint. [<a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>SpanBERT: Improving Pre-training by Representing and Predicting Spans</strong>. <em>Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy</em>. Preprint. [<a href="https://arxiv.org/pdf/1907.10529.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/facebookresearch/SpanBERT" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>Knowledge Enhanced Contextual Word Representations</strong>. <em>Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A. Smith</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.04164.pdf" target="_blank" rel="noopener">pdf</a>] (<strong>KnowBert</strong>) </li>
<li><strong>VisualBERT: A Simple and Performant Baseline for Vision and Language</strong>. <em>Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang</em>. Preprint. [<a href="https://arxiv.org/pdf/1908.03557.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/uclanlp/visualbert" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</strong>. <em>Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee</em>. NeurIPS 2019. [<a href="https://arxiv.org/pdf/1908.02265.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/jiasenlu/vilbert_beta" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>VideoBERT: A Joint Model for Video and Language Representation Learning</strong>. <em>Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid</em>. ICCV 2019. [<a href="https://arxiv.org/pdf/1904.01766.pdf" target="_blank" rel="noopener">pdf</a>] </li>
<li><strong>LXMERT: Learning Cross-Modality Encoder Representations from Transformers</strong>. <em>Hao Tan, Mohit Bansal</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1908.07490.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/airsplay/lxmert" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>VL-BERT: Pre-training of Generic Visual-Linguistic Representations</strong>. <em>Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai</em>. Preprint. [<a href="https://arxiv.org/pdf/1908.08530.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</strong>. <em>Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou</em>. Preprint. [<a href="https://arxiv.org/pdf/1908.06066.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>K-BERT: Enabling Language Representation with Knowledge Graph</strong>. <em>Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, Ping Wang</em>. Preprint. [<a href="https://arxiv.org/pdf/1909.07606.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Fusion of Detected Objects in Text for Visual Question Answering</strong>. <em>Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1908.05054.pdf" target="_blank" rel="noopener">pdf</a>] (<strong>B2T2</strong>)</li>
<li><strong>Contrastive Bidirectional Transformer for Temporal Representation Learning</strong>. <em>Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid</em>. Preprint. [<a href="https://arxiv.org/pdf/1906.05743.pdf" target="_blank" rel="noopener">pdf</a>] (<strong>CBT</strong>)</li>
<li><strong>ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</strong>. <em>Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, Haifeng Wang</em>. Preprint. [<a href="https://arxiv.org/pdf/1907.12412v1.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/PaddlePaddle/ERNIE/blob/develop/README.md" target="_blank" rel="noopener">code</a>] </li>
<li><strong>75 Languages, 1 Model: Parsing Universal Dependencies Universally</strong>. <em>Dan Kondratyuk, Milan Straka</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1904.02099.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/hyperparticle/udify" target="_blank" rel="noopener">code &amp; model</a>] (<strong>UDify</strong>)</li>
<li><strong>Pre-Training with Whole Word Masking for Chinese BERT</strong>. <em>Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu</em>. Preprint. [<a href="https://arxiv.org/pdf/1906.08101.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md" target="_blank" rel="noopener">code &amp; model</a>] (<strong>Chinese-BERT-wwm</strong>)</li>
<li><strong>UNITER: Learning UNiversal Image-TExt Representations</strong>. <em>Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu</em>. Preprint. [<a href="https://arxiv.org/pdf/1909.11740.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>HUBERT Untangles BERT to Improve Transfer across NLP Tasks</strong>. <em>Anonymous authors</em>. ICLR 2020 under review. [<a href="https://openreview.net/pdf?id=HJxnM1rFvr" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</strong>.  <em>Julian Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin Kardas, Sylvain Gugger, Jeremy Howard</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.04761.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="http://nlp.fast.ai/classification/2019/09/10/multifit.html" target="_blank" rel="noopener">code &amp; model</a>]</li>
</ol>
<h3 id="Knowledge-Distillation-amp-Model-Compression"><a href="#Knowledge-Distillation-amp-Model-Compression" class="headerlink" title="Knowledge Distillation &amp; Model Compression"></a>Knowledge Distillation &amp; Model Compression</h3><ol>
<li><strong>TinyBERT: Distilling BERT for Natural Language Understanding</strong>. <em>Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu</em>. Preprint. [<a href="https://arxiv.org/pdf/1909.10351v2.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</strong>. <em>Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin</em>. Preprint. [<a href="https://arxiv.org/pdf/1903.12136.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Patient Knowledge Distillation for BERT Model Compression</strong>. <em>Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1908.09355.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/intersun/PKD-for-BERT-Model-Compression" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System</strong>. <em>Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang</em>. Preprint. [<a href="https://arxiv.org/pdf/1904.09636.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation</strong>. <em>Wei Zhu, Xiaofeng Zhou, Keqiang Wang, Xun Luo, Xiepeng Li, Yuan Ni, Guotong Xie</em>. The 18th BioNLP workshop. [<a href="https://www.aclweb.org/anthology/W19-5040" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</strong>. <em>Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao</em>. Preprint. [<a href="https://arxiv.org/pdf/1904.09482.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener">code &amp; model</a>]</li>
<li><strong>Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation</strong>. <em>Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</em>. Preprint. [<a href="https://arxiv.org/pdf/1908.08962.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Small and Practical BERT Models for Sequence Labeling</strong>. <em>Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, Amelia Archer</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.00100.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</strong>. <em>Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer</em>. Preprint. [<a href="https://arxiv.org/pdf/1909.05840.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</strong>.  <em>Anonymous authors</em>. ICLR 2020 under review. [<a href="https://openreview.net/pdf?id=H1eA7AEtvS" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Extreme Language Model Compression with Optimal Subwords and Shared Projections</strong>. <em>Sanqiang Zhao, Raghav Gupta, Yang Song, Denny Zhou</em>. Preprint. [<a href="https://arxiv.org/pdf/1909.11687" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</strong>. <em>Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf</em>. Preprint. [<a href="https://arxiv.org/pdf/1910.01108" target="_blank" rel="noopener">pdf</a>]</li>
</ol>
<h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><ol>
<li><strong>Revealing the Dark Secrets of BERT</strong>. <em>Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky</em>. EMNLP 2019. [<a href="https://arxiv.org/abs/1908.08593" target="_blank" rel="noopener">pdf</a>] </li>
<li><strong>How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</strong>. <em>Betty van Aken, Benjamin Winter, Alexander Löser, Felix A. Gers</em>. CIKM 2019. [<a href="https://arxiv.org/pdf/1909.04925.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Are Sixteen Heads Really Better than One?</strong>. <em>Paul Michel, Omer Levy, Graham Neubig</em>. Preprint. [<a href="https://arxiv.org/pdf/1905.10650.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/pmichel31415/are-16-heads-really-better-than-1" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</strong>. <em>Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits</em>. Preprint. [<a href="https://arxiv.org/pdf/1907.11932.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/jind11/TextFooler" target="_blank" rel="noopener">code</a>]</li>
<li><strong>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</strong>. <em>Alex Wang, Kyunghyun Cho</em>. NeuralGen 2019. [<a href="https://arxiv.org/pdf/1902.04094.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/nyu-dl/bert-gen" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Linguistic Knowledge and Transferability of Contextual Representations</strong>. <em>Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith</em>. NAACL 2019. [<a href="https://www.aclweb.org/anthology/N19-1112" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>What Does BERT Look At? An Analysis of BERT’s Attention</strong>. <em>Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning</em>. BlackBoxNLP 2019. [<a href="https://arxiv.org/pdf/1906.04341.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/clarkkev/attention-analysis" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Open Sesame: Getting Inside BERT’s Linguistic Knowledge</strong>. <em>Yongjie Lin, Yi Chern Tan, Robert Frank</em>. BlackBoxNLP 2019. [<a href="https://arxiv.org/pdf/1906.01698.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/yongjie-lin/bert-opensesame" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Analyzing the Structure of Attention in a Transformer Language Model</strong>. <em>Jesse Vig, Yonatan Belinkov</em>. BlackBoxNLP 2019. [<a href="https://arxiv.org/pdf/1906.04284.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains</strong>. <em>Samira Abnar, Lisa Beinborn, Rochelle Choenni, Willem Zuidema</em>. BlackBoxNLP 2019. [<a href="https://arxiv.org/pdf/1906.01539.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>BERT Rediscovers the Classical NLP Pipeline</strong>. <em>Ian Tenney, Dipanjan Das, Ellie Pavlick</em>. ACL 2019. [<a href="https://www.aclweb.org/anthology/P19-1452" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>How multilingual is Multilingual BERT?</strong>. <em>Telmo Pires, Eva Schlinger, Dan Garrette</em>. ACL 2019. [<a href="https://www.aclweb.org/anthology/P19-1493" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>What Does BERT Learn about the Structure of Language?</strong>. <em>Ganesh Jawahar, Benoît Sagot, Djamé Seddah</em>. ACL 2019. [<a href="https://www.aclweb.org/anthology/P19-1356" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</strong>. <em>Shijie Wu, Mark Dredze</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1904.09077.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</strong>. <em>Kawin Ethayarajh</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.00512.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Probing Neural Network Comprehension of Natural Language Arguments</strong>. <em>Timothy Niven, Hung-Yu Kao</em>. ACL 2019. [<a href="https://www.aclweb.org/anthology/P19-1459" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/IKMLab/arct2" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Universal Adversarial Triggers for Attacking and Analyzing NLP</strong>. <em>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1908.07125.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/Eric-Wallace/universal-triggers" target="_blank" rel="noopener">code</a>]</li>
<li><strong>The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</strong>. <em>Elena Voita, Rico Sennrich, Ivan Titov</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.01380.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Do NLP Models Know Numbers? Probing Numeracy in Embeddings</strong>. <em>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.07940.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs</strong>. <em>Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mohananey, Phu Mon Htut, Paloma Jeretič, Samuel R. Bowman</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1909.02597.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/alexwarstadt/data_generation" target="_blank" rel="noopener">code</a>]</li>
<li><strong>Visualizing and Understanding the Effectiveness of BERT</strong>. <em>Yaru Hao, Li Dong, Furu Wei, Ke Xu</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1908.05620.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Visualizing and Measuring the Geometry of BERT</strong>. <em>Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, Martin Wattenberg</em>. NeurIPS 2019. [<a href="https://arxiv.org/pdf/1906.02715.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>On the Validity of Self-Attention as Explanation in Transformer Models</strong>. <em>Gino Brunner, Yang Liu, Damián Pascual, Oliver Richter, Roger Wattenhofer</em>. Preprint. [<a href="https://arxiv.org/pdf/1908.04211.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel</strong>. <em>Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, Ruslan Salakhutdinov</em>. EMNLP 2019. [<a href="https://arxiv.org/pdf/1908.11775.pdf" target="_blank" rel="noopener">pdf</a>]</li>
<li><strong>Language Models as Knowledge Bases?</strong> <em>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel</em>. EMNLP 2019, [<a href="https://arxiv.org/pdf/1909.01066.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/facebookresearch/LAMA" target="_blank" rel="noopener">code</a>]</li>
<li><strong>To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</strong>. <em>Matthew E. Peters, Sebastian Ruder, Noah A. Smith</em>. RepL4NLP 2019, [<a href="https://www.aclweb.org/anthology/W19-4302.pdf" target="_blank" rel="noopener">pdf</a>]</li>
</ol>
<h2 id="Tutorial-amp-Resource"><a href="#Tutorial-amp-Resource" class="headerlink" title="Tutorial &amp; Resource"></a>Tutorial &amp; Resource</h2><ol>
<li><strong>Transfer Learning in Natural Language Processing</strong>. <em>Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, Thomas Wolf</em>. NAACL 2019. [<a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?usp=sharing" target="_blank" rel="noopener">slides</a>] </li>
<li><strong>Transformers: State-of-the-art Natural Language Processing</strong>. <em>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew</em>. Preprint. [<a href="https://arxiv.org/pdf/1910.03771.pdf" target="_blank" rel="noopener">pdf</a>] [<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">code</a>]</li>
</ol>

    
    </div>
    
    
        <div class="article-comment" id='article-comment'>
            

<h1>评论</h1>

  
    <div id='gitment'></div>
  


        </div>
        
</article>
  </div>

  

<footer id='footer'>
    <div class='footer-copyright'>
        <div>
            <p> 版权所有 <a href="">times </a> @ 2019</p>
            <p>设计: <i class="fas fa-paint-brush"></i> <a href="https://moober.cn">Moober</a> 和 <i class="fas fa-graduation-cap"></i> <a href="https://qutang.github.io">Qu Tang</a> &bull; 主题: <a href="https://qutang.github.io/cutie/">Cutie 2.1.3-Taurus</a> &bull; 由 <a href="http://hexo.io">Hexo.</a> 强力驱动</p>
        </div>
    </div>
    
    <div class='footer-social'>
        
    </div>
</footer>

  <br>

  <div id="footer-nav" class='footer-nav'>
		



<nav id="nav">
	
	
	
	<div class='nav-item' id='nav-item-toc'>
		


<div class='toc-container'>
<i class="far fa-times-circle" id='toc-close' onclick='closeTOC(event);' ontouchstart='closeTOC(event);'></i>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PLMpapers"><span class="toc-number">1.</span> <span class="toc-text">PLMpapers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Papers"><span class="toc-number">1.2.</span> <span class="toc-text">Papers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tutorial-amp-Resource"><span class="toc-number">1.3.</span> <span class="toc-text">Tutorial &amp; Resource</span></a></li></ol></li></ol>
</div>
<div class="toc-button" onclick='toggleTOC(event);' ontouchstart='toggleTOC(event);'>
    <img src="/images/icons/blue-shadow/toc.svg" alt="">
</div>

	</div>
	
	<div class='nav-item' id='nav-item-archive'>
		
				<div class='nav-icon'>
				
			<a href="/archives/" title='归档'>
			<img src="/images/icons/blue-shadow/archive.svg" alt="">
			</a>
		</div>
	</div>
	<div class='nav-item' id='nav-item-search'>
		
		<div class='nav-icon'>
		
			<a href="/search/" title='搜索'>
			<img src="/images/icons/blue-shadow/search.svg" alt="">
			</a>
		</div>
	</div>
	<div class="nav-item" id='nav-item-more'>
		<div class="nav-icon">
				<a href='#' onclick='onClickMenuIcon(event);' ontouchstart='onClickMenuIcon(event);'>
				<img src="/images/icons/blue-shadow/menu.svg" alt="">
				</a>
		</div>
		<div class="nav-more-menu">
				<i class="far fa-times-circle" id='nav-more-menu-close' onclick='onClickNavMenuClose(event);' ontouchstart='onClickNavMenuClose(event);'></i>
		
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/文章/">
						<span>文章</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/算法/">
						<span>算法</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/人工智能/">
						<span>人工智能</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/生活/">
						<span>生活</span>
					</a>
				</div>
		</div>
		
		<div class='nav-more-item'>
				<div class="nav-name">
					<a class='nav-link' href="/categories/笔试/">
						<span>笔试</span>
					</a>
				</div>
		</div>
		
	</div>
	</div>
</nav>

	</div>

  



    






  <script>
  var gitment = new Gitment({
    id: window.location.pathname, // optional
    owner: 'hsutimes',
    repo: 'gitment_blog',
    oauth: {
      client_id: 'd73d768ea9e3bd69ab54',
      client_secret: '4b958932671ae1a2ab37b2a112ee945824fd4abd',
    },
    // ...
    // For more available options, check out the documentation below
  })

  gitment.render('gitment')
  </script>







    <script src="/js/lightgallery.min.js"></script>
<script src="/js/lg-zoom.min.js"></script>
<script type='text/javascript'>
    $(document).ready(function() {
        $("#lightgallery").lightGallery(); 
        $(".article-content img").each(function(){
            console.log($(this).attr('src'))
            $(this).attr('data-src', $(this).attr('src')).lightGallery({
                selector: 'this'
            })
        });
    });
</script>






<script type='text/javascript'>

  
  // update cookie if this page is opened (directly)
  loadjs(['/libs/jshashes/hashes.min.js', '/libs/js-cookie/src/js.cookie.js', '/js/post.v2.js'], 'post-version');
  loadjs.ready('post-version', function(){
    
    new Postv2('hashit_3719374984689d9f37eedf1be18cbb74a00337abc849c277f18628dfd87fba98').update('hashit_57d5c1b8cca608eaa303c20b95e0be52498f221715a979fe39fa36f4284c70c0', function(){});
  });
  
</script>


    
<script type='text/javascript'>
  
  // update cookie if this page is opened (directly)
  function getIP(json) {
  loadjs(['/libs/jshashes/hashes.min.js', '/libs/js-cookie/src/js.cookie.js', '/js/leancloud.js'], 'post-visit-comment-count');
  loadjs.ready('post-visit-comment-count', function(){
    
    
    LeanCloud.init('x8sFMx0q5JkG3Y3x6Q3wiVoe-gzGzoHsz', 'Dt5xQxK8HtOYoNAmGQoWVMa6');
    var leanCloud = LeanCloud.getInstance();
    leanCloud.fetchIsThumbUp('/2019/10/20/PLMpapers/', json.ip);
    document.getElementById('thumb-up-button').addEventListener('click', function(e){
      leanCloud.isThumbUp('/2019/10/20/PLMpapers/', function(isThumbUp){
        console.log('is thumb up:' + isThumbUp);
        if(isThumbUp){
          leanCloud.removeThumbUpRecord('/2019/10/20/PLMpapers/', json.ip);
          document.getElementById('thumb-up-icon').className = "far fa-thumbs-up fa-lg";
          
        }else{
          leanCloud.addThumbUpRecord('/2019/10/20/PLMpapers/', json.ip);
          document.getElementById('thumb-up-icon').className = "fas fa-thumbs-up fa-lg";
        }
        leanCloud.getThumbUpCount('/2019/10/20/PLMpapers/', function(count){
            console.log('thumb up count: ' + count)
            var el = document.getElementById('article-thumbup-count');
            if(el) el.innerHTML = count;
        });
      });
    });
    leanCloud.addVisitRecord('/2019/10/20/PLMpapers/', json.ip);
    leanCloud.fetchCommentCount('/2019/10/20/PLMpapers/');
    leanCloud.fetchVisitCount('/2019/10/20/PLMpapers/');
    leanCloud.fetchThumbUpCount('/2019/10/20/PLMpapers/');
    leanCloud.getCommentCount('/2019/10/20/PLMpapers/', function(count){
        var el = document.querySelector('#article-comment-count');
        if(el) el.innerHTML = count;
    });
    leanCloud.getVisitCount('/2019/10/20/PLMpapers/', function(count){
        var el = document.querySelector('#article-visit-count');
        if(el) el.innerHTML = count;
    });
    leanCloud.getThumbUpCount('/2019/10/20/PLMpapers/', function(count){
        var el = document.getElementById('article-thumbup-count');
        if(el) el.innerHTML = count;
    });
    leanCloud.isThumbUp('/2019/10/20/PLMpapers/', function(isThumbUp){
        console.log('init thumb up:' + isThumbUp);
        if(isThumbUp){
          document.getElementById('thumb-up-icon').className = "fas fa-thumbs-up fa-lg";
        }else{
          document.getElementById('thumb-up-icon').className = "far fa-thumbs-up fa-lg";
        }
    });
  });
  }
  
</script>

<script type="application/javascript" src="https://api.ipify.org?format=jsonp&callback=getIP"></script>



<!-- <script src="/js/post.js"></script> -->

<script src="/js/headroom.min.js"></script>

<script data-no-instant type='text/javascript'>

initHeadroom();

changeLayoutOnTouchScreen();

// 
// var post = new Post('x8sFMx0q5JkG3Y3x6Q3wiVoe-gzGzoHsz', 'Dt5xQxK8HtOYoNAmGQoWVMa6');
// post.getCommentCount(window.location.pathname, function(count){
//     $('#article-comment-count').text(count);
// });
// post.addVisitRecord(window.location.pathname, userip);
// post.getVisitCount(window.location.pathname, function(count){
//     $('#article-visit-count').text(count);
// });

// 
</script>


<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
